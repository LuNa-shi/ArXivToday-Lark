# Configuration

# ==================================== Feishu(Lark) Configuration ==================================== #
# ------------------------------------------------------------------------------------------------------------ #
# Feishu(Lark) Bot Webhook URL
webhook_url: 'https://open.feishu.cn/open-apis/bot/v2/hook/7d813083-5f05-4507-9267-e8fb4266214d'

# Feishu(Lark) Card Template
# `ArXivToday.card` can be imported into Lark as a card template.
# After import, replace the fields below with the generated values from Lark.
template_id: 'AAq2Qvj02QJWh'  # TODO: Replace with template_id generated after importing ArXivToday.card
template_version_name: '1.0.1'  # TODO: Replace with template_version_name from Lark template
# ------------------------------------------------------------------------------------------------------------ #


# ==================================== Paper Configuration ==================================== #
# ------------------------------------------------------------------------------------------------------------ #
tag: 'VLN & VLA'  # Tag for Feishu(Lark) Card

category_list:  # arXiv categories to search for papers
  - cs.CL  # Computation and Language
  - cs.AI  # Artificial Intelligence
  - cs.CV  # Computer Vision and Pattern Recognition
  - cs.RO
  - cs.LG


keyword_list:  # Keywords to filter papers
  - vision-language navigation
  - vision language navigation
  - VLN
  - embodied navigation
  - embodied ai
  - object navigation
  - objectnav
  - visual navigation
  - navigation policy
  - vision-language-action
  - vision language action
  - VLA
  - robot policy
  - action model
# ------------------------------------------------------------------------------------------------------------ #


# ==================================== LLM Service Configuration ==================================== #
# ------------------------------------------------------------------------------------------------------------ #

# LLM Server Config
# model: 'deepseek-r1:32b'
# base_url: 'http://localhost:11434/v1'  # NOTE: For ollama, need to add '/v1' at the end of the OLLAMA_HOST URL
# api_key: 'ollama'

#### >>> LLM Server Config EXAMPLE >>> ####

## 1. For Ollama Server ##
# model: 'qwen2.5:7b'
# base_url: 'http://localhost:11434/v1'  # NOTE: For ollama, need to add '/v1' at the end of the OLLAMA_HOST URL
# api_key: 'ollama'  # Any non-empty string works (Ollama does not require authentication)

## 2. For Other OpenAI SDK-Compatibale LLM Server ##
model: 'qwen/qwen3-235b-a22b-2507'
base_url: 'https://openrouter.ai/api/v1'
api_key: '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'

#### <<< LLM Server Config EXAMPLE <<< ####

# ------------------------------------------------------------------------------------------------------------ #

# Use LLM for More Accurate Paper Filtering
use_llm_for_filtering: true  # Set to false to disable LLM-based filtering

#### >>> LLM-Based Paper Filtering >>> ####
# If set to true, `paper_to_hunt.md` file in the project root directory will be used for LLM-based filtering.
# You can modify the prompt in `paper_to_hunt.md` to describe the paper you want to hunt for.
#
# If you want to use LLM-Based Filtering **only** (without Keyword Filtering), set `keyword_list` to an empty list like below:
# keyword_list: []
#### <<< LLM-Based Paper Filtering <<< ####

# ------------------------------------------------------------------------------------------------------------ #

# Use LLM for Paper Abstract Translation
use_llm_for_translation: true  # Set to false to disable LLM-based translation

# ------------------------------------------------------------------------------------------------------------ #
